{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Description\n",
    "\n",
    "This lab aims to introduce you to the practical considerations when performing machine learning in the real world. Real data is generally not clean. We may be given data with gaps, outliers or irrelevant features. For models to perform well, we need to do some data cleaning to fix these issues, and decide what to consider a feature for the machine learning, and what to drop. \n",
    "\n",
    "> __Problem Statement__: Your aim is to predict if a given person survived the Titanic's maiden voyage given some limited information about them.\n",
    "\n",
    "## Competition.\n",
    "This lab is based on a famous Kaggle competition. In the spirit of the famous Kaggle competition, this lab will also be a competition!\n",
    "\n",
    "#### Competition Rules: \n",
    "- No cheating! It would be against the spirit of the competition to search for the test data on Kaggle (or elsewhere), for instance! Searching the names of Titanic survivors would also be considered cheating!\n",
    "- The winner will be the entry with the highest score submitted before the end of the lab on the unseen test data.\n",
    "- To optimise hyperparameters, you must perform a grid search, random search, Bayesian optimisation, or some other optimisation technique. Specifically, entries where you have stumbled upon good hyperparameters by chance will be considered invalid.\n",
    "- No parallelization, please! Certain algorithms allow you to run on multiple CPU cores. We are running on a shared server with limited resource, so please do not attempt to parallelise your training. For example, do not set `n_jobs` to any value (keep the default of 1).\n",
    "- Entries that \"luck out\" and achieve a very high score against the test data despite having a modest score on the validation data will be considered invalid.\n",
    "  \n",
    "#### Prize: \n",
    "- Pride!\n",
    "- A small prize (please do not get too excited; it really will be nominal!)\n",
    "\n",
    "\n",
    "## MATLAB vs. Python\n",
    "I understand you have been trained in MATLAB, but I want to put you out of your comfort zone a little and encourage you to use Python for this lab. Firstly, it'll be good for your CV to pick up a few Python skills. Secondly, the entire machine-learning community is centred around Python, so it would be amiss of us not to use it in AERO40041 too.\n",
    "\n",
    "If you struggle, please do feel free to explore the dataset in MATLAB instead, but please do try the Python code first. We're also here to help you and have several GTAs on hand who know Python very well should you need it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "\n",
    "Pandas is used to open the csv file, explore its contents, and start to perform some feature cleanup. In Python, it is common to import a package while giving the package a shortened name. For example, in the cell below, we write `import pandas as pd` which sets `pd` as an alias for `pandas` so we can write `pd.read_csv(<filename>)` rather than `pandas.read_csv(<filename>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from seaborn) (1.26.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use Pandas to load the training data as a Pandas dataframe called df. The `head` method of the df object prints the first few lines of the file. Study the data format and familiarise yourself with the type of data we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Shorney, Mr. Charles Joseph</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>374910</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Goldschmidt, Mr. George B</td>\n",
       "      <td>male</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17754</td>\n",
       "      <td>34.6542</td>\n",
       "      <td>A5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Greenfield, Mr. William Bertram</td>\n",
       "      <td>male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17759</td>\n",
       "      <td>63.3583</td>\n",
       "      <td>D10 D12</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Doling, Mrs. John T (Ada Julia Bone)</td>\n",
       "      <td>female</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>231919</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Kantor, Mr. Sinai</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>244367</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Survived  Pclass  \\\n",
       "0             1         0       3   \n",
       "1             2         1       1   \n",
       "2             3         1       3   \n",
       "3             4         1       1   \n",
       "4             5         0       3   \n",
       "..          ...       ...     ...   \n",
       "95           96         0       3   \n",
       "96           97         0       1   \n",
       "97           98         1       1   \n",
       "98           99         1       2   \n",
       "99          100         0       2   \n",
       "\n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       "0                             Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1   Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                              Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                            Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                ...     ...   ...    ...   \n",
       "95                        Shorney, Mr. Charles Joseph    male   NaN      0   \n",
       "96                          Goldschmidt, Mr. George B    male  71.0      0   \n",
       "97                    Greenfield, Mr. William Bertram    male  23.0      0   \n",
       "98               Doling, Mrs. John T (Ada Julia Bone)  female  34.0      0   \n",
       "99                                  Kantor, Mr. Sinai    male  34.0      1   \n",
       "\n",
       "    Parch            Ticket     Fare    Cabin Embarked  \n",
       "0       0         A/5 21171   7.2500      NaN        S  \n",
       "1       0          PC 17599  71.2833      C85        C  \n",
       "2       0  STON/O2. 3101282   7.9250      NaN        S  \n",
       "3       0            113803  53.1000     C123        S  \n",
       "4       0            373450   8.0500      NaN        S  \n",
       "..    ...               ...      ...      ...      ...  \n",
       "95      0            374910   8.0500      NaN        S  \n",
       "96      0          PC 17754  34.6542       A5        C  \n",
       "97      1          PC 17759  63.3583  D10 D12        C  \n",
       "98      1            231919  23.0000      NaN        S  \n",
       "99      0            244367  26.0000      NaN        S  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('TitanicTrainVal.csv')\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass                             Name   Sex  \\\n",
      "179          180         0       3              Leonard, Mr. Lionel  male   \n",
      "271          272         1       3     Tornquist, Mr. William Henry  male   \n",
      "302          303         0       3  Johnson, Mr. William Cahoone Jr  male   \n",
      "597          598         0       3              Johnson, Mr. Alfred  male   \n",
      "\n",
      "      Age  SibSp  Parch Ticket  Fare Cabin Embarked  \n",
      "179  36.0      0      0    NaN   0.0   NaN        S  \n",
      "271  25.0      0      0    NaN   0.0   NaN        S  \n",
      "302  19.0      0      0    NaN   0.0   NaN        S  \n",
      "597  49.0      0      0    NaN   0.0   NaN        S  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 887 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  887 non-null    int64  \n",
      " 1   Survived     887 non-null    int64  \n",
      " 2   Pclass       887 non-null    int64  \n",
      " 3   Name         887 non-null    object \n",
      " 4   Sex          887 non-null    object \n",
      " 5   Age          710 non-null    float64\n",
      " 6   SibSp        887 non-null    int64  \n",
      " 7   Parch        887 non-null    int64  \n",
      " 8   Ticket       887 non-null    object \n",
      " 9   Fare         887 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     885 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 90.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df['Ticket'] = df['Ticket'].str.extract(r'(\\d{1})', expand=False)\n",
    "print(df[df['Ticket'].isnull()])\n",
    "df = df.dropna(subset=['Ticket'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more detailed description about the meaning of each attribute and its values:\n",
    "\n",
    "* PassengerId - A column added by Kaggle to uniquely identify each row and make submissions easier\n",
    "* Survived - Whether the passenger survived or not (0=No, 1=Yes) __(*)__\n",
    "* Pclass - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd).\n",
    "* Name - The passenger's name and title.\n",
    "* Sex - The passenger's sex\n",
    "* Age - The passenger's age in years\n",
    "* SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n",
    "* Parch - The number of parents or children the passenger had aboard the Titanic\n",
    "* Ticket - The passenger's ticket number\n",
    "* Fare - The fare the passenger paid in £\n",
    "* Cabin - The passenger's cabin number\n",
    "* Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)\n",
    "\n",
    "---\n",
    "\n",
    "__(*)__ This is what we aim to predict. This is training (or validation) data, so has this label. The test data will have the label hidden.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "Let's explore the data. The dataset dimensions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 12)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means there are 891 samples, and 12 columns (PassengerId, Survived, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the presence of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 887 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  887 non-null    int64  \n",
      " 1   Survived     887 non-null    int64  \n",
      " 2   Pclass       887 non-null    int64  \n",
      " 3   Name         887 non-null    object \n",
      " 4   Sex          887 non-null    object \n",
      " 5   Age          710 non-null    float64\n",
      " 6   SibSp        887 non-null    int64  \n",
      " 7   Parch        887 non-null    int64  \n",
      " 8   Ticket       887 non-null    object \n",
      " 9   Fare         887 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     885 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 90.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most columns have 891 entries, but Age, Cabin and Embarked all have some missing entries. We can compute the percentage of missing value for each potential feature as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.954904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.001127</td>\n",
       "      <td>0.225479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Name  Sex        Age  SibSp  Parch  Ticket  \\\n",
       "0          0.0       0.0     0.0   0.0  0.0  19.954904    0.0    0.0     0.0   \n",
       "\n",
       "   Fare      Cabin  Embarked  \n",
       "0   0.0  77.001127  0.225479  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.isnull().sum()/df.shape[0]*100).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~20% of Age entries, ~77% of cabin entries and ~0.2% of Embarked entries are missing. We will need to decide what to do about this shortly.\n",
    "\n",
    "We can also compute statistics for each column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>887.000000</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>710.000000</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>887.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.485908</td>\n",
       "      <td>0.384442</td>\n",
       "      <td>2.305524</td>\n",
       "      <td>29.684746</td>\n",
       "      <td>0.525366</td>\n",
       "      <td>0.383315</td>\n",
       "      <td>32.349436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.617131</td>\n",
       "      <td>0.486738</td>\n",
       "      <td>0.836662</td>\n",
       "      <td>14.540819</td>\n",
       "      <td>1.104669</td>\n",
       "      <td>0.807466</td>\n",
       "      <td>49.758238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>669.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   887.000000  887.000000  887.000000  710.000000  887.000000   \n",
       "mean    446.485908    0.384442    2.305524   29.684746    0.525366   \n",
       "std     257.617131    0.486738    0.836662   14.540819    1.104669   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     447.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     669.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  887.000000  887.000000  \n",
       "mean     0.383315   32.349436  \n",
       "std      0.807466   49.758238  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.925000  \n",
       "50%      0.000000   14.458300  \n",
       "75%      0.000000   31.137500  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare the Data\n",
    "\n",
    "Now we will perform some data cleaning and preparation. You can consider this as pre-processing the data before any machine learning is performed to get it ready for learning. What you do here is up to you! The score you achieve against the held-out test data is likely to be strongly dependent on how well you prepare the data.\n",
    "\n",
    "---\n",
    "__Suggestions__\n",
    "\n",
    "1. Data cleaning:\n",
    "    * Fix or remove outliers \n",
    "    * Fill in missing values (e.g., with zero, mean, median...) or drop their rows (or columns).\n",
    "2. Feature selection:\n",
    "    * Drop the attributes that provide no useful information for the task.\n",
    "3. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features.\n",
    "    * Decompose features (e.g., categorical, date/time, etc.).\n",
    "    * Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).\n",
    "    * Aggregate features into promising new features.\n",
    "\n",
    "4. Feature scaling: standardize or normalize features.\n",
    "\n",
    "\n",
    "In the cells below, we perform some basic data preparation. __It is likely you can do better than this by following some of the suggestions above.__ Our approach below is a simple one.\n",
    "\n",
    "---\n",
    "\n",
    "> Note: In the cells below, we will be dropping selected rows, columns, generating new columns, etc. These edits will be performed on the original dataframe. If you want to go back to the original to try something new, you can simply re-read the CSV file by running `df = pd.read_csv('TitanicTrainVal.csv')`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Dealing with ``Cabin`` and ``Embarked``__\n",
    "\n",
    "Since ``Cabin`` has 77% of missing values, we will drop the entire column.\n",
    "\n",
    "Analogously we can drop the few NaN instances corresponding to the ``Embarked`` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(885, 11)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Cabin', axis=1)\n",
    "df = df.dropna(subset=['Embarked'], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we now have 889 samples (because the few without `Embarked` have been dropped). We also now only have 11 columns (because `Cabin` was dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with ``Sex`` and ``Embarked``__\n",
    "\n",
    "These columns are categorical. It  is better to have them as a one-hot-encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>...</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Ticket_1</th>\n",
       "      <th>Ticket_2</th>\n",
       "      <th>Ticket_3</th>\n",
       "      <th>Ticket_4</th>\n",
       "      <th>Ticket_5</th>\n",
       "      <th>Ticket_6</th>\n",
       "      <th>Ticket_7</th>\n",
       "      <th>Ticket_8</th>\n",
       "      <th>Ticket_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry  35.0      0      0   \n",
       "\n",
       "      Fare  Sex_female  Sex_male  ...  Embarked_S  Ticket_1  Ticket_2  \\\n",
       "0   7.2500       False      True  ...        True     False     False   \n",
       "1  71.2833        True     False  ...       False      True     False   \n",
       "2   7.9250        True     False  ...        True     False      True   \n",
       "3  53.1000        True     False  ...        True      True     False   \n",
       "4   8.0500       False      True  ...        True     False     False   \n",
       "\n",
       "   Ticket_3  Ticket_4  Ticket_5  Ticket_6  Ticket_7  Ticket_8  Ticket_9  \n",
       "0     False     False      True     False     False     False     False  \n",
       "1     False     False     False     False     False     False     False  \n",
       "2     False     False     False     False     False     False     False  \n",
       "3     False     False     False     False     False     False     False  \n",
       "4      True     False     False     False     False     False     False  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked','Ticket'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with ``Ticket``__\n",
    "\n",
    "The ticket column is unlikely to be useful as there is no obvious link between the ticket number and survival rate (e.g. `A/5 21171` and `PC 17599` are selected values). We will drop it for now (you can decide what to do with it if you want; maybe there is some subtle information in there that is useful). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket_1</th>\n",
       "      <th>Ticket_2</th>\n",
       "      <th>Ticket_3</th>\n",
       "      <th>Ticket_4</th>\n",
       "      <th>Ticket_5</th>\n",
       "      <th>Ticket_6</th>\n",
       "      <th>Ticket_7</th>\n",
       "      <th>Ticket_8</th>\n",
       "      <th>Ticket_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ticket_1  Ticket_2  Ticket_3  Ticket_4  Ticket_5  Ticket_6  Ticket_7  \\\n",
       "0     False     False     False     False      True     False     False   \n",
       "1      True     False     False     False     False     False     False   \n",
       "2     False      True     False     False     False     False     False   \n",
       "3      True     False     False     False     False     False     False   \n",
       "4     False     False      True     False     False     False     False   \n",
       "\n",
       "   Ticket_8  Ticket_9  \n",
       "0     False     False  \n",
       "1     False     False  \n",
       "2     False     False  \n",
       "3     False     False  \n",
       "4     False     False  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Name`` column may bring some interesting information. For example, Mr., Miss, Mrs., Maester, etc. may have some correlation on the rate of survival. This might help improve the model, but for now, we drop it for simplicity as it is not a trivial column to use. The salutation is also likely to be highly correlated with the `Sex` column, and somewhat correlated with the `Age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df = df.drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also drop PassengerId since this is just a label Kaggle added to uniquely identify the passenger, and has no correlation with survival rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('PassengerId', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values\n",
    "\n",
    "The ``Age`` column has some missing values. We can try to fill them by using the average age (this is not necessarily the best option, but is perhaps a reasonable one. It is left to you to investigate this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = df['Age'].mean()\n",
    "\n",
    "df['Age'] = df['Age'].fillna(mean_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check we have no missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (889, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 889 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    889 non-null    int64  \n",
      " 1   Pclass      889 non-null    int64  \n",
      " 2   Age         889 non-null    float64\n",
      " 3   SibSp       889 non-null    int64  \n",
      " 4   Parch       889 non-null    int64  \n",
      " 5   Fare        889 non-null    float64\n",
      " 6   Sex_female  889 non-null    bool   \n",
      " 7   Sex_male    889 non-null    bool   \n",
      " 8   Embarked_C  889 non-null    bool   \n",
      " 9   Embarked_Q  889 non-null    bool   \n",
      " 10  Embarked_S  889 non-null    bool   \n",
      "dtypes: bool(5), float64(2), int64(4)\n",
      "memory usage: 53.0 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape: \", df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the features and labels\n",
    "Now we have cleaned the data, we are ready to generate our features (X) and label (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Survived'].copy() \n",
    "X = df.loc[:, df.columns != 'Survived'].copy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the machine learning package `scikit-learn` to perform the machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into training data (which we use to optimise the parameters) and validation data (which we use to optimise the hyperparameters). The function `train_test_split` does this with a random selection. Despite the function's name, the split data should be considered as training data and validation data (not test data). We also have some held-out test data with hidden labels, which you will use to get your final score. The argument `test_size=0.2` means we select a random 20% of the original data as our validation data. You may change this if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(711, 10) (711,) (178, 10) (178,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val,  y_train, y_val = train_test_split(X, y, random_state=123, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will try several classification algorithms.\n",
    "\n",
    "### __Logistic Regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the documentation of the SGDClassifier class by running the cell below. Pay attention to the arguments the function can take. Most of them have default values, but the defaults will not necessarily work well for every problem. It is very likely you will need to perform a hyperparameter search to achieve good performance. If you do not understand certain parts of the documentation, it is likely because we had not covered it in AERO40041. You may ask a GTA if you are curious or just stick to the algorithms we have covered.\n",
    "\n",
    "---\n",
    "\n",
    "> Note: Running the cell below produces a lot of text. Depending on your version of Jupyter, you may want to click to the left of the cell after running it to collapse the ouput into a smaller scrollable window (this may be the default).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hinge'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'optimal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpower_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSource:\u001b[0m        \n",
      "\u001b[1;32mclass\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseSGDClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"\"\"Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      "\n",
      "    This estimator implements regularized linear models with stochastic\n",
      "    gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "    each sample at a time and the model is updated along the way with a\n",
      "    decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "    (online/out-of-core) learning via the `partial_fit` method.\n",
      "    For best results using the default learning rate schedule, the data should\n",
      "    have zero mean and unit variance.\n",
      "\n",
      "    This implementation works with data represented as dense or sparse arrays\n",
      "    of floating point values for the features. The model it fits can be\n",
      "    controlled with the loss parameter; by default, it fits a linear support\n",
      "    vector machine (SVM).\n",
      "\n",
      "    The regularizer is a penalty added to the loss function that shrinks model\n",
      "    parameters towards the zero vector using either the squared euclidean norm\n",
      "    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "    parameter update crosses the 0.0 value because of the regularizer, the\n",
      "    update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "    online feature selection.\n",
      "\n",
      "    Read more in the :ref:`User Guide <sgd>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    loss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',\\\n",
      "        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',\\\n",
      "        'squared_epsilon_insensitive'}, default='hinge'\n",
      "        The loss function to be used.\n",
      "\n",
      "        - 'hinge' gives a linear SVM.\n",
      "        - 'log_loss' gives logistic regression, a probabilistic classifier.\n",
      "        - 'modified_huber' is another smooth loss that brings tolerance to\n",
      "          outliers as well as probability estimates.\n",
      "        - 'squared_hinge' is like hinge but is quadratically penalized.\n",
      "        - 'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "        - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n",
      "          'squared_epsilon_insensitive' are designed for regression but can be useful\n",
      "          in classification as well; see\n",
      "          :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      "\n",
      "        More details about the losses formulas can be found in the\n",
      "        :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "\n",
      "    penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n",
      "        The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "        which is the standard regularizer for linear SVM models. 'l1' and\n",
      "        'elasticnet' might bring sparsity to the model (feature selection)\n",
      "        not achievable with 'l2'. No penalty is added when set to `None`.\n",
      "\n",
      "    alpha : float, default=0.0001\n",
      "        Constant that multiplies the regularization term. The higher the\n",
      "        value, the stronger the regularization. Also used to compute the\n",
      "        learning rate when `learning_rate` is set to 'optimal'.\n",
      "        Values must be in the range `[0.0, inf)`.\n",
      "\n",
      "    l1_ratio : float, default=0.15\n",
      "        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "        Only used if `penalty` is 'elasticnet'.\n",
      "        Values must be in the range `[0.0, 1.0]`.\n",
      "\n",
      "    fit_intercept : bool, default=True\n",
      "        Whether the intercept should be estimated or not. If False, the\n",
      "        data is assumed to be already centered.\n",
      "\n",
      "    max_iter : int, default=1000\n",
      "        The maximum number of passes over the training data (aka epochs).\n",
      "        It only impacts the behavior in the ``fit`` method, and not the\n",
      "        :meth:`partial_fit` method.\n",
      "        Values must be in the range `[1, inf)`.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    tol : float or None, default=1e-3\n",
      "        The stopping criterion. If it is not None, training will stop\n",
      "        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "        epochs.\n",
      "        Convergence is checked against the training loss or the\n",
      "        validation loss depending on the `early_stopping` parameter.\n",
      "        Values must be in the range `[0.0, inf)`.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    shuffle : bool, default=True\n",
      "        Whether or not the training data should be shuffled after each epoch.\n",
      "\n",
      "    verbose : int, default=0\n",
      "        The verbosity level.\n",
      "        Values must be in the range `[0, inf)`.\n",
      "\n",
      "    epsilon : float, default=0.1\n",
      "        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "        For 'huber', determines the threshold at which it becomes less\n",
      "        important to get the prediction exactly right.\n",
      "        For epsilon-insensitive, any differences between the current prediction\n",
      "        and the correct label are ignored if they are less than this threshold.\n",
      "        Values must be in the range `[0.0, inf)`.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        The number of CPUs to use to do the OVA (One Versus All, for\n",
      "        multi-class problems) computation.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "\n",
      "    random_state : int, RandomState instance, default=None\n",
      "        Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "        Integer values must be in the range `[0, 2**32 - 1]`.\n",
      "\n",
      "    learning_rate : str, default='optimal'\n",
      "        The learning rate schedule:\n",
      "\n",
      "        - 'constant': `eta = eta0`\n",
      "        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "          where `t0` is chosen by a heuristic proposed by Leon Bottou.\n",
      "        - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "        - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\n",
      "          Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "          training loss by tol or fail to increase validation score by tol if\n",
      "          `early_stopping` is `True`, the current learning rate is divided by 5.\n",
      "\n",
      "            .. versionadded:: 0.20\n",
      "                Added 'adaptive' option\n",
      "\n",
      "    eta0 : float, default=0.0\n",
      "        The initial learning rate for the 'constant', 'invscaling' or\n",
      "        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "        the default schedule 'optimal'.\n",
      "        Values must be in the range `[0.0, inf)`.\n",
      "\n",
      "    power_t : float, default=0.5\n",
      "        The exponent for inverse scaling learning rate.\n",
      "        Values must be in the range `(-inf, inf)`.\n",
      "\n",
      "    early_stopping : bool, default=False\n",
      "        Whether to use early stopping to terminate training when validation\n",
      "        score is not improving. If set to `True`, it will automatically set aside\n",
      "        a stratified fraction of training data as validation and terminate\n",
      "        training when validation score returned by the `score` method is not\n",
      "        improving by at least tol for n_iter_no_change consecutive epochs.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "            Added 'early_stopping' option\n",
      "\n",
      "    validation_fraction : float, default=0.1\n",
      "        The proportion of training data to set aside as validation set for\n",
      "        early stopping. Must be between 0 and 1.\n",
      "        Only used if `early_stopping` is True.\n",
      "        Values must be in the range `(0.0, 1.0)`.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "            Added 'validation_fraction' option\n",
      "\n",
      "    n_iter_no_change : int, default=5\n",
      "        Number of iterations with no improvement to wait before stopping\n",
      "        fitting.\n",
      "        Convergence is checked against the training loss or the\n",
      "        validation loss depending on the `early_stopping` parameter.\n",
      "        Integer values must be in the range `[1, max_iter)`.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "            Added 'n_iter_no_change' option\n",
      "\n",
      "    class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "        Preset for the class_weight fit parameter.\n",
      "\n",
      "        Weights associated with classes. If not given, all classes\n",
      "        are supposed to have weight one.\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "    warm_start : bool, default=False\n",
      "        When set to True, reuse the solution of the previous call to fit as\n",
      "        initialization, otherwise, just erase the previous solution.\n",
      "        See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "        Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "        result in a different solution than when calling fit a single time\n",
      "        because of the way the data is shuffled.\n",
      "        If a dynamic learning rate is used, the learning rate is adapted\n",
      "        depending on the number of samples already seen. Calling ``fit`` resets\n",
      "        this counter, while ``partial_fit`` will result in increasing the\n",
      "        existing counter.\n",
      "\n",
      "    average : bool or int, default=False\n",
      "        When set to `True`, computes the averaged SGD weights across all\n",
      "        updates and stores the result in the ``coef_`` attribute. If set to\n",
      "        an int greater than 1, averaging will begin once the total number of\n",
      "        samples seen reaches `average`. So ``average=10`` will begin\n",
      "        averaging after seeing 10 samples.\n",
      "        Integer values must be in the range `[1, n_samples]`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else \\\n",
      "            (n_classes, n_features)\n",
      "        Weights assigned to the features.\n",
      "\n",
      "    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "        Constants in decision function.\n",
      "\n",
      "    n_iter_ : int\n",
      "        The actual number of iterations before reaching the stopping criterion.\n",
      "        For multiclass fits, it is the maximum over every binary fit.\n",
      "\n",
      "    loss_function_ : concrete ``LossFunction``\n",
      "\n",
      "        .. deprecated:: 1.4\n",
      "            Attribute `loss_function_` was deprecated in version 1.4 and will be\n",
      "            removed in 1.6.\n",
      "\n",
      "    classes_ : array of shape (n_classes,)\n",
      "\n",
      "    t_ : int\n",
      "        Number of weight updates performed during training.\n",
      "        Same as ``(n_iter_ * n_samples + 1)``.\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    sklearn.svm.LinearSVC : Linear support vector classification.\n",
      "    LogisticRegression : Logistic regression.\n",
      "    Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      "        ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      "        penalty=None)``.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.linear_model import SGDClassifier\n",
      "    >>> from sklearn.preprocessing import StandardScaler\n",
      "    >>> from sklearn.pipeline import make_pipeline\n",
      "    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "    >>> Y = np.array([1, 1, 2, 2])\n",
      "    >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "    >>> clf = make_pipeline(StandardScaler(),\n",
      "    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      "    >>> clf.fit(X, Y)\n",
      "    Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                    ('sgdclassifier', SGDClassifier())])\n",
      "    >>> print(clf.predict([[-0.8, -1]]))\n",
      "    [1]\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m**\u001b[0m\u001b[0mBaseSGDClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"penalty\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"l2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"l1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"elasticnet\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"alpha\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"l1_ratio\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"both\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"power_t\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"neither\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"epsilon\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"constant\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"optimal\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"invscaling\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"adaptive\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mHidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"pa1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pa2\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"eta0\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"hinge\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"l2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEFAULT_EPSILON\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"optimal\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mpower_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mpower_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpower_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"log_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"modified_huber\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m\"probability estimates are not available for loss=%r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mavailable_if\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"Probability estimates.\n",
      "\n",
      "        This method is only available for log loss and modified Huber loss.\n",
      "\n",
      "        Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "        estimates by simple normalization, as recommended by Zadrozny and\n",
      "        Elkan.\n",
      "\n",
      "        Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "        it is necessary to perform proper probability calibration by wrapping\n",
      "        the classifier with\n",
      "        :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "            Input data for prediction.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        ndarray of shape (n_samples, n_classes)\n",
      "            Returns the probability of the sample for each class in the model,\n",
      "            where classes are ordered as they are in `self.classes_`.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "        probability estimates\", SIGKDD'02,\n",
      "        https://dl.acm.org/doi/pdf/10.1145/775047.775151\n",
      "\n",
      "        The justification for the formula in the loss=\"modified_huber\"\n",
      "        case is in the appendix B in:\n",
      "        http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "        \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"log_loss\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"modified_huber\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mprob\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mprob\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob2\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# the above might assign zero to all classes, which doesn't\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# normalize neatly; work around this to produce uniform\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# probabilities\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mall_zero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob_sum\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_zero\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_zero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mprob_sum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_zero\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# normalize\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mprob_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m\"predict_(log_)proba only supported when\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m\" loss='log_loss' or loss='modified_huber' \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m\"(%r given)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mavailable_if\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mpredict_log_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"Log of probability estimates.\n",
      "\n",
      "        This method is only available for log loss and modified Huber loss.\n",
      "\n",
      "        When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "        and ones, so taking the logarithm is not possible.\n",
      "\n",
      "        See ``predict_proba`` for details.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input data for prediction.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        T : array-like, shape (n_samples, n_classes)\n",
      "            Returns the log-probability of the sample for each class in the\n",
      "            model, where classes are ordered as they are in\n",
      "            `self.classes_`.\n",
      "        \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m\"_xfail_checks\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m\"check_sample_weights_invariance\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\"zero sample_weight is not equivalent to removing samples\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m\"preserves_dtype\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\amiru\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "SGDClassifier??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `SGDClassifier` uses stochastic gradient descent to train various linear algorithms. To use logistic regression, we need to set `loss='log_loss'` (otherwise, we are using a different machine learning algorithm to Logistic regression). The argument `random_state=42` is useful to ensure repeatable results. The specific number 42  is arbitrary (42 is the answer to life, the universe, and everything according to Douglas Adams' comedic science fiction series, \"The Hitchhiker's Guide to the Galaxy.\"). Finally, we set learning_rate=`constant` to use a fixed learning rate throughout the training. This is chosen for simplicity. Other options are `optimal` and `adaptive` which dynamically set the learning rate at each update of the parameters according to the schedule described in the documentation in the cell above. The specific value we use is set by the argument `eta0=1e-3`. (Note that scikit-learn uses `eta` to mean learning rate, whereas we had used `alpha` in the lectures. Both are common nomenclature). To further add to the confusion, the regularisation hyperparameter ($\\lambda$ in the lectures) is referred to as `alpha` in scikit learn! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=0.01, eta0=0.1, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;,\n",
       "              random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SGDClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SGDClassifier(alpha=0.01, eta0=0.1, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;,\n",
       "              random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(alpha=0.01, eta0=0.1, learning_rate='constant', loss='log_loss',\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg = SGDClassifier(loss='log_loss', learning_rate='constant', eta0=0.1, alpha=0.0001, random_state=42)\n",
    "LogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained a logistic regression model! Let's see how it did __on the validation data__ (not test data yet as we have not optimised hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = LogReg.predict(X_val)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.41 means we only got 41% of predictions correct! This is pretty terrible (in fact, we might on average have expected to do better by just guessing!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning\n",
    "In the cells below, we will perform a basic hyperparameter search to get you started. This is not exhaustive, but may do better than 41% accuracy (we hope!). Scikit-learn has a few options for hyperparameter searches built in. These use k-fold cross-validation. For more info, see [here](https://scikit-learn.org/1.5/api/sklearn.model_selection.html#hyper-parameter-optimizers). However, since we have an explicit validation set created above, we will implement our own simple grid search rather than use the k-fold cross-validation. A random search may also be useful instead of a grid search (see [ParameterSampler](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.ParameterSampler.html#sklearn.model_selection.ParameterSampler) to get started on implementing a random search). \n",
    "\n",
    "We will attempt to optimise the regularisation type and the regularisation parameter $\\lambda$ (i.e. $\\alpha$ in scikit-learn). You can search over other hyperparameters.\n",
    "\n",
    "---\n",
    "> Note: Please do not run excessive hyperparameter searches that take more than a few minutes to run, as the server may not be able to handle the load. If your cell takes more than 5 minutes to complete, please interrupt the calculation by clicking Kernel -> interrupt and retry a more modest hyperparameter search. The number of individual models trained will equal the number of options to test for hyperparameter 1 multiplied by the number of options to test for hyperparameter 2, etc. = 9*3 = 27 models in the cell below.). Limit the number of models to <100. You can start with a coarse hyperparameter search, and then fine-tune it once you have narrowed down the range. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param: {'alpha': 0.0001, 'penalty': 'l2'} validation accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', None],\n",
    "    'alpha' : np.logspace(-7, 1, 9)\n",
    "}\n",
    "\n",
    "best_accuracy = 0\n",
    "for params in ParameterGrid(param_grid):\n",
    "    LogReg = SGDClassifier(**params, loss='log_loss', learning_rate='constant', eta0=0.1, random_state=42)\n",
    "    LogReg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = LogReg.predict(X_val)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    if( accuracy > best_accuracy ):\n",
    "        best_accuracy = accuracy\n",
    "        best_model = LogReg\n",
    "        best_params = params\n",
    "        \n",
    "print(\"Best Param:\", best_params, f\"validation accuracy: {best_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improvement in accuracy is a step in the right direction, but you can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking on the test data (hidden labels) and submitting scores.\n",
    "To make predictions on the test data, you will need to provide a function that prepares the test data in exactly the same way as you did for the training/validation data. The cell below will therefore need to change to reflect the pre-processing you actually did (if you did something different to the simple preprocessing we proposed above). It is vital that you perform exactly the same steps as you did to prepare your data for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess( df ):\n",
    "    df = df.drop('Cabin', axis=1)\n",
    "    df = df.dropna(subset=['Embarked'], axis=0)\n",
    "    df = pd.get_dummies(df, columns=['Sex', 'Embarked'])\n",
    "    df = df.drop('Ticket', axis=1)\n",
    "    df = df.drop('Name', axis=1)\n",
    "    df = df.drop('PassengerId', axis=1)\n",
    "    df['Age'] = df['Age'].fillna(mean_age)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You pass the model and the preprocessing function to our testing function via `AERO40041.testTitanicModel(model, preprocessing_function)`. The testing function is hosted on PyPI so can be installed using `pip install AERO40041` if you are not using the MaSC-portal. We are providing this test function rather than the test data directly to remove the risk of data leakage (where you accidentally train your model on the test data due to a subtle bug). Some method of complete segregation/isolation between the test set and training set is generally recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AERO40041\n",
    "\n",
    "accuracy = AERO40041.testTitanicModel(best_model, preprocess)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try:\n",
    "- Scikit-learn provides other classification models with the same call sequence of `model.fit` and `model.predict`. This makes it very easy to try other models. You may try a neural network (`from sklearn.neural_network import MLPClassifier`) or a decision tree (`from sklearn.tree import DecisionTreeClassifier`), for instance, or something else!\n",
    "- Each model will have its own set of hyperparameters. You can learn more about them by running a cell like this `MLPClassifier??` or `DecisionTreeClassifier??`.\n",
    "- Different choices of feature cleanup or feature engineering is likely to yield significant gains. Play around with this.\n",
    "- Once you have your best score, share it with a GTA to enter the competition!\n",
    "- How does your own neural network developed in Lab 1 perform?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
